# ğŸ§  LLM from Scratch â€” Python Implementation

[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)  
[![Python Version](https://img.shields.io/badge/python-%3E%3D3.8-%3C4.0-blue.svg)](#)  
[![Project Status](https://img.shields.io/badge/status-learning-proof-of-concept-yellow.svg)](#)

This repository contains a **from-scratch implementation of a Large Language Model (LLM)** in Python â€” built for educational purposes and clarity of understanding.

It follows the workflow of the YouTube lecture series ğŸ¥ **[â€œLLM from Scratchâ€](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu)**

---

## ğŸ” Whatâ€™s Inside

- Minimal Transformer architecture: tokenisation â†’ embeddings â†’ positional encoding â†’ self-attention â†’ feed-forward â†’ generation  
- Readable code, no heavy abstractionâ€”ideal for learning how LLMs work internally  
- Focused on clarity and education, not on production-scale performance  

---

## ğŸ“º Reference

Lecture series: [https://www.youtube.com/playlist?list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu)

---

## ğŸ“œ License

This project is released under the **MIT License**.

